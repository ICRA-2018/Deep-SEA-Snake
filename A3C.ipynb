{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronus Advantage Actor-Critic (A3C) on a Snake Robot\n",
    "\n",
    "This iPython notebook includes an implementation of the [A3C algorithm](https://arxiv.org/pdf/1602.01783.pdf), based on Arthur Juliani's A3C code. For more information on A3C, see the accompanying [Medium post](https://medium.com/p/c88f72a5e9f2/edit).\n",
    "\n",
    "This version runs A3C with 6 workers on the offline database \"p_experiments.snake\" (which needs to be downloaded separately, see README.md), which was obtained on a real snake robot running the state-of-the-art compliant controller with 6 windows.\n",
    "\n",
    "While training is taking place, statistics on agent performance are available from Tensorboard. To launch it use:\n",
    "\n",
    "`tensorboard --logdir=worker_0:'./train_W_0',worker_1:'./train_W_1',worker_2:'./train_W_2',worker_3:'./train_W_3',worker_4:'./train_W_4',worker_5:'./train_W_5'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"p_experiments.snake\",\"rb\") as f:\n",
    "    exps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=4, micro=6, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import SnakeEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "print(sys.version_info)\n",
    "import multiprocessing\n",
    "import threading\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_GRAPH = True\n",
    "LOG_DIR = './log'\n",
    "N_WORKERS = 6 #multiprocessing.cpu_count()\n",
    "MAX_GLOBAL_EP = 50000\n",
    "GLOBAL_NET_SCOPE = 'Global_Net'\n",
    "UPDATE_GLOBAL_ITER = 89\n",
    "GAMMA = 0.995\n",
    "ENTROPY_BETA = 0.01\n",
    "LR_A = 1e-4    # learning rate for actor\n",
    "LR_C = 1e-4    # learning rate for critic\n",
    "GLOBAL_REWARD = []\n",
    "GLOBAL_EP = 0\n",
    "model_path = './model_offlineA3C'\n",
    "load_model = False\n",
    "episode_length = 89\n",
    "\n",
    "\n",
    "s_size = 7\n",
    "a_size = 9\n",
    "\n",
    "os.system('rm -Rf log train_W* '+model_path)\n",
    "os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Actor-Critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ACNet(object):\n",
    "    def __init__(self, scope, globalAC=None):\n",
    "\n",
    "        if scope == GLOBAL_NET_SCOPE:   # Only need parameters of global network\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, s_size], 'S')\n",
    "                self._build_net()\n",
    "                self.a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "                self.c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        else:   # Uselocal net to calculate losses\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, s_size], 'S')\n",
    "                self.a_his = tf.placeholder(tf.int32, [None, ], 'A')\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "                \n",
    "                #Get policy(a_prob) and value(v) from actor, critic net\n",
    "                self.a_prob, self.v = self._build_net()\n",
    "\n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                with tf.name_scope('q_loss'):\n",
    "                    self.c_loss = tf.reduce_mean(tf.square(td))\n",
    "\n",
    "                with tf.name_scope('a_loss'):\n",
    "                    #use clip to avoid log(0): tf.clip_by_value(self.policy, 1e-20, 1.0)\n",
    "                    log_prob = tf.reduce_sum(tf.log(tf.clip_by_value(self.a_prob, 1e-20, 1.0)) * tf.one_hot(self.a_his, a_size, dtype=tf.float32), axis=1, keep_dims=True)\n",
    "                    exp_v = log_prob * td\n",
    "                    #Found someone using entropy to encourage exploration\n",
    "                    #larger entropy means more stochastic actions\n",
    "                    entropy = -tf.reduce_sum(self.a_prob * tf.log(tf.clip_by_value(self.a_prob, 1e-20, 1.0)), axis=1, keep_dims=True)\n",
    "                    self.exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "                    self.a_loss = tf.reduce_mean(-self.exp_v)\n",
    "\n",
    "                with tf.name_scope('local_grad'):\n",
    "                    self.a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "                    self.c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "                    self.a_grads = tf.gradients(self.a_loss, self.a_params)\n",
    "                    self.c_grads = tf.gradients(self.c_loss, self.c_params)\n",
    "                \n",
    "            # Syncronization\n",
    "            with tf.name_scope('sync'):\n",
    "                #assign global parameters to local parameters\n",
    "                with tf.name_scope('pull'):\n",
    "                    self.pull_a_params_op = [local_param.assign(global_param) for local_param, global_param in zip(self.a_params, globalAC.a_params)]\n",
    "                    self.pull_c_params_op = [local_param.assign(global_param) for local_param, global_param in zip(self.c_params, globalAC.c_params)]\n",
    "                #update params of global net by pushing the calculated gradients of local net to global net    \n",
    "                with tf.name_scope('push'):\n",
    "                    self.update_a_op = trainer_A.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
    "                    self.update_c_op = trainer_C.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
    "\n",
    "    def _build_net(self):\n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        with tf.variable_scope('actor'):\n",
    "            layer4 = tf.layers.dense(self.s, a_size, tf.nn.relu6, kernel_initializer=w_init, name='layer4')\n",
    "            layer3 = tf.layers.dense(layer4, a_size, tf.nn.relu6, kernel_initializer=w_init, name='layer3')\n",
    "            layer2 = tf.layers.dense(layer3, a_size, tf.nn.relu6, kernel_initializer=w_init, name='layer2')\n",
    "            a_prob = tf.layers.dense(layer2, a_size, tf.nn.softmax, kernel_initializer=w_init, name='ap')\n",
    "        with tf.variable_scope('critic'):\n",
    "            v4 = tf.layers.dense(self.s, 1, tf.nn.relu6, kernel_initializer=w_init, name='v4')\n",
    "            v3 = tf.layers.dense(v4, 1, tf.nn.relu6, kernel_initializer=w_init, name='v3')\n",
    "            v2 = tf.layers.dense(v3, 1, tf.nn.relu6, kernel_initializer=w_init, name='v2')\n",
    "            v = tf.layers.dense(v2, 1, tf.nn.relu6, kernel_initializer=w_init, name='v')\n",
    "        return a_prob, v\n",
    "\n",
    "    def update_global(self, feed_dict):  # run by a local\n",
    "        a_grads,c_grads,c_loss,a_loss,_,_ = SESS.run([self.a_grads,self.c_grads,self.c_loss, self.a_loss,self.update_a_op, self.update_c_op], feed_dict)# local grads applies to global net\n",
    "        return a_loss,c_loss,a_grads,c_grads\n",
    "    def pull_global(self):  # run by a local\n",
    "        SESS.run([self.pull_a_params_op, self.pull_c_params_op])\n",
    "\n",
    "    def choose_action(self, s):  # run by a local\n",
    "        prob_weights = SESS.run(self.a_prob, feed_dict={self.s: np.array(s)})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),\n",
    "                                  p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, name, workerID, globalAC):\n",
    "        self.workerID = workerID\n",
    "        self.env = SnakeEnvironment.SnakeEnv(exps,workerID,89)\n",
    "        self.name = name\n",
    "        self.AC = ACNet(name, globalAC)\n",
    "        self.model_path = model_path\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.name))        \n",
    "\n",
    "    def work(self):\n",
    "        global GLOBAL_REWARD, GLOBAL_EP\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            s,_,_ = self.env.reset()\n",
    "            ep_r = 0\n",
    "            ep_step=0\n",
    "            ep_action_counter=0\n",
    "            a_loss, c_loss = [], []            \n",
    "            while True:\n",
    "                a,_ = self.env.action_space.sample()\n",
    "                s1, r, done,_ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append(r)\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0:   # update global and assign to local net\n",
    "                    if done:\n",
    "                        v_s1 = 0   # terminal\n",
    "                    else:\n",
    "                        v_s1 = SESS.run(self.AC.v, {self.AC.s: np.array(s1)})[0, 0]\n",
    "                        assert not(np.any(np.isnan(v_s1)))\n",
    "                    buffer_v_target = []\n",
    "                    for r in buffer_r[::-1]:    # reverse buffer r\n",
    "                        v_s1 = r + GAMMA * v_s1\n",
    "                        buffer_v_target.append(v_s1)\n",
    "                    buffer_v_target.reverse()\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.array(buffer_a), np.vstack(buffer_v_target)\n",
    "                    feed_dict = {\n",
    "                        self.AC.s: buffer_s,\n",
    "                        self.AC.a_his: buffer_a,\n",
    "                        self.AC.v_target: buffer_v_target,\n",
    "                    }\n",
    "                    a_l,c_l,a_grads,c_grads = self.AC.update_global(feed_dict)\n",
    "                    a_loss.append(a_l)\n",
    "                    c_loss.append(c_l)\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                    self.AC.pull_global()\n",
    "\n",
    "                s = s1\n",
    "                total_step += 1\n",
    "                ep_step +=1\n",
    "                if ep_step == episode_length:\n",
    "                    GLOBAL_REWARD.append(ep_r)\n",
    "\n",
    "                    print(\n",
    "                        self.name,\n",
    "                        \"Ep:\", GLOBAL_EP,\n",
    "                        \"| Ep_r: %i\" % GLOBAL_REWARD[-1]\n",
    "                          )\n",
    "                    if GLOBAL_EP % 100 == 0:\n",
    "                        saver.save(SESS,self.model_path+'/model-'+str(GLOBAL_EP)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "                    if GLOBAL_EP % 5 ==0:\n",
    "                        mean_reward = np.mean(GLOBAL_REWARD[-5:])\n",
    "                        mean_c_loss = np.max(np.mean(c_loss[-5:]))\n",
    "                        mean_a_loss = np.max(np.mean(a_loss[-5:]))\n",
    "                        summary=tf.Summary()\n",
    "                        summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                        summary.value.add(tag='Losses/Value Loss', simple_value=float(mean_c_loss))\n",
    "                        summary.value.add(tag='Losses/Policy Loss', simple_value=float(mean_a_loss))\n",
    "                        self.summary_writer.add_summary(summary, GLOBAL_EP)\n",
    "\n",
    "                        self.summary_writer.flush()\n",
    "                    GLOBAL_EP += 1\n",
    "                    \n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.system(\"rm -r train_W*\")\n",
    "    SESS = tf.Session()\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        trainer_A = tf.train.AdamOptimizer(LR_A, name='RMSPropA')\n",
    "        trainer_C = tf.train.AdamOptimizer(LR_C, name='RMSPropC')\n",
    "        GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # we only need its params\n",
    "        workers = []\n",
    "        # Create worker\n",
    "        for i in range(N_WORKERS):#N_WORKERS\n",
    "            i_name = 'W_%i' % i   # worker name\n",
    "            workers.append(Worker(i_name, i, GLOBAL_AC))\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    COORD = tf.train.Coordinator()\n",
    "    if load_model==True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(SESS,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    if OUTPUT_GRAPH:\n",
    "        if os.path.exists(LOG_DIR):\n",
    "            shutil.rmtree(LOG_DIR)\n",
    "        tf.summary.FileWriter(LOG_DIR, SESS.graph)\n",
    "\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        job = lambda: worker.work()\n",
    "        t = threading.Thread(target=job)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "    COORD.join(worker_threads)\n",
    "\n",
    "    plt.plot(np.arange(len(GLOBAL_REWARD)), GLOBAL_REWARD)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('Total moving reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
